{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDA Freediving Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project DeepPlay aims at exploring and displaying the world of competitive freediving using web-scraping, machine learning and data visualizations (e.g. D3.js). The main source of information is the official website of AIDA, International Association for the Development of Apnea. The present work has been created within 10 days including exploratory data analysis.\n",
    "\n",
    "1- Scraping the data from the website\n",
    "\n",
    "2- Data preparation / cleaning / extension (separate name / country, get GPS locations, get gender...)\n",
    "\n",
    "3- Early data exploration (see exploratory_data_analysis.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method get_discipline_value(key) selects one of 6 disciplines (dictionary keys: STA, DYN, DNF, CWT, CNF, FIM) and allocates its corresponding value (id) to a new url, discipline_url.\n",
    "If the discipline is mispelled or inexistent, get_discipline_value throws the sentence \"Check your spelling ... is not a freediving discipline\".\n",
    "\n",
    "The method is called within the following method scraper( ) function to obtain html pages associated with a discipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_discipline_value(key):\n",
    "\n",
    "    disc = {'STA': 8 ,\n",
    "        'DYN': 6,\n",
    "        'DNF': 7,\n",
    "        'CWT': 3,\n",
    "        'CNF': 4,\n",
    "        'FIM': 5\n",
    "        }\n",
    "    if key in disc.keys():\n",
    "        value = disc[key]\n",
    "        discipline_url = '{}{}'.format('&disciplineId=', value) \n",
    "        return discipline_url\n",
    "    else:\n",
    "        logging.warning('Check your spelling. ' + key + ' is not a freediving discipline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_discipline_value('NFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method cleanser( ) changes the list of lists named 'data' which is collected all html pages for each discipline into a cleaned and labelled dataframe df. The method uses regular expressions. It will also be called within the method scraper( )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanser(a_list):\n",
    "    \n",
    "    df = pd.DataFrame(a_list)\n",
    "    df.columns = ['Ranking', 'Name', 'Results', 'Announced', 'Points', 'Penalties', 'Date', 'Place']\n",
    "    df['Ranking'] = df['Ranking'].str.replace('.', '')\n",
    "    df['Country'] = df['Name'].str.extract('.*\\((.*)\\).*', expand=True)\n",
    "    df['Name'] = df['Name'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "    df['Results'] = df['Results'].str.replace('m', '')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.drop_duplicates(['Name', 'Results', 'Announced', 'Points', 'Penalties', 'Date', 'Place', 'Country'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method scraper( ) crawls through an entire freediving discipline, identifies how many pages it consists of (max_pages), obtains html code from all urls and save this code into a list of lists (data). The later is saved into a cleaned data frame using cleanser( ), ready for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scraper(key):\n",
    "    \n",
    "    #Obtain html code for url and Parse the page\n",
    "    base_url = 'https://www.aidainternational.org/Ranking/Rankings?page='\n",
    "    url = '{}1{}'.format(base_url, get_discipline_value(key))\n",
    "\n",
    "    page = rq.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "\n",
    "    #Use regex to identify the maximum number of pages for the discipline of interest\n",
    "    page_count = soup.findAll(text=re.compile(r\"Page .+ of .+\"))\n",
    "    max_pages = str(page_count).split(' ')[3].split('\\\\')[0]\n",
    "    total_obs = int(max_pages)*20\n",
    "\n",
    "    data = []\n",
    "    for p in range(1, int(max_pages)+1):\n",
    "\n",
    "        #For each page, create corresponding url, request the library, obtain html code and parse the page\n",
    "        url = '{}{}{}'.format(base_url, p, get_discipline_value(key))\n",
    "\n",
    "        #The break plays the role of safety guard if dictionary key is wrong (not spelled properly or non-existent) then the request\n",
    "        #for library is not executed (and not going through the for loop to generate the data), an empty dataframe is saved\n",
    "        if url == '{}{}None'.format(base_url, p):\n",
    "            break\n",
    "        else:\n",
    "            new_page = rq.get(url)\n",
    "            new_soup = BeautifulSoup(new_page.content, \"lxml\")\n",
    "\n",
    "            #For each page, each parsed page is saved into the list named \"data\"\n",
    "            rows = new_soup.table.tbody.findAll('tr')\n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                cols = [ele.text.strip() for ele in cols]\n",
    "                data.append([ele for ele in cols if ele])\n",
    "\n",
    "            p += 1\n",
    "\n",
    "    #Results from list \"data\" are cleaned using \"cleanser\" method and saved in a dataframe clean_df\n",
    "    clean_df = cleanser(data)\n",
    "    pd.set_option('max_rows', int(total_obs))\n",
    "    pd.set_option('expand_frame_repr', True)\n",
    "\n",
    "    #Dataframe df is saved in file results_key.csv to access results offline\n",
    "    filename = '/Users/fabienplisson/Desktop/Github_shares/DeepPlay/deepplay/data/cleaned/results_{}.csv'.format(key)\n",
    "    clean_df.to_csv(filename, encoding ='utf-8')\n",
    "    logging.warning('Finished!')\n",
    "    #with open(filename,'a') as f:\n",
    "        #f.write(clean_df.encode('uft-8'))\n",
    "    #f.closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scraper('DYN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Integrating all methods into class using Object-oriented programming (OOP)\n",
    "- Tidying up data with more specific regular expressions\n",
    "- Applying web-scraping to other websites to collect other features and datasets that share similar types of information (athlete name, country, record values (time, distance), location of the event, date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
