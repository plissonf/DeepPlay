{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDA Freediving Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scope of that project is explore and display all freediving records from one of the world organization of freediving, AIDA in a series of D3.js visualizations. Such plateform could be extended to other datasets that share similar types of information (athlete name, country, record values (time, distance), location of the event, date).\n",
    "\n",
    "1- Scraping the data from the website\n",
    "\n",
    "2- Data preparation / cleaning / extension (separate name / country, get GPS locations, get gender...)\n",
    "\n",
    "3- Early data exploration\n",
    "\n",
    "4- D3.js data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_discipline_value(key):\n",
    "    '''This function selects one of 6 disciplines (dictionary keys) and allocates its corresponding value (id)\n",
    "    to discipline_url. The function is called in scraper() function to obtain corresponding html pages'''\n",
    "\n",
    "    disc = {'STA': 8 ,\n",
    "        'DYN': 6,\n",
    "        'DNF': 7,\n",
    "        'CWT': 3,\n",
    "        'CNF': 4,\n",
    "        'FIM': 5\n",
    "        }\n",
    "    if key in disc.keys():\n",
    "        value = disc[key]\n",
    "        discipline_url = '{}{}'.format('&disciplineId=', value) \n",
    "        return discipline_url\n",
    "    else:\n",
    "        logging.warning('Check your spelling. ' + key + ' is not a freediving discipline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_discipline_value('NFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanser(a_list):\n",
    "    '''This function changes the list named 'data' scrapped for each discipline and turns it\n",
    "    into a cleaned and labelled dataframe df using regular expression.'''\n",
    "    \n",
    "    df = pd.DataFrame(a_list)\n",
    "    df.columns = ['Ranking', 'Name', 'Results', 'Announced', 'Points', 'Penalties', 'Date', 'Place']\n",
    "    df['Ranking'] = df['Ranking'].str.replace('.', '')\n",
    "    df['Country'] = df['Name'].str.extract('.*\\((.*)\\).*', expand=True)\n",
    "    df['Name'] = df['Name'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "    df['Results'] = df['Results'].str.replace('m', '')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.drop_duplicates(['Name', 'Results', 'Announced', 'Points', 'Penalties', 'Date', 'Place', 'Country'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scraper(key):\n",
    "    ''' This function crawls through an entire freediving discipline, regardless how many pages it consists of. '''\n",
    "\n",
    "    #Obtain html code for url and Parse the page\n",
    "    base_url = 'https://www.aidainternational.org/Ranking/Rankings?page='\n",
    "    url = '{}1{}'.format(base_url, get_discipline_value(key))\n",
    "\n",
    "    page = rq.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "\n",
    "    #Use regex to identify the maximum number of pages for the discipline of interest\n",
    "    page_count = soup.findAll(text=re.compile(r\"Page .+ of .+\"))\n",
    "    max_pages = str(page_count).split(' ')[3].split('\\\\')[0]\n",
    "\n",
    "    data = []\n",
    "    for p in range(1, int(max_pages)+1):\n",
    "\n",
    "        #For each page, create corresponding url, request the library, obtain html code and parse the page\n",
    "        url = '{}{}{}'.format(base_url, p, get_discipline_value(key))\n",
    "\n",
    "        #The break plays the role of safety guard if dictionary key is wrong (not spelled properly or non-existent) then the request\n",
    "        #for library is not executed (and not going through the for loop to generate the data), an empty dataframe is saved\n",
    "        if url == '{}{}None'.format(base_url, p):\n",
    "            break\n",
    "        else:\n",
    "            new_page = rq.get(url)\n",
    "            new_soup = BeautifulSoup(new_page.content, \"lxml\")\n",
    "\n",
    "            #For each page, each parsed page is saved into the list named \"data\"\n",
    "            rows = new_soup.table.tbody.findAll('tr')\n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                cols = [ele.text.strip() for ele in cols]\n",
    "                data.append([ele for ele in cols if ele])\n",
    "\n",
    "            p += 1\n",
    "\n",
    "    #Results from list \"data\" are cleaned using \"cleanser\" method and saved in a dataframe clean_df\n",
    "    clean_df = cleanser(data)\n",
    "    logging.warning('Finished!')\n",
    "\n",
    "    #Dataframe df is saved in file results.txt to access results offline\n",
    "    filename = '/Users/fabienplisson/Desktop/Github_shares/DeepPlay/deepplay/data/cleaned/results_{}.txt'.format(key)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(str(clean_df))\n",
    "    f.closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scraper('STA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
